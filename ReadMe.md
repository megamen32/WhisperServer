# WhisperServer

A high-performance transcription server using [Faster Whisper](https://github.com/guillaumekln/faster-whisper), built with FastAPI, supporting:

- âœ… On-the-fly model selection (`tiny`, `base`, `small`, etc.)
- âš¡ Priority-based queue (lightweight models are processed first)
- ğŸ’¾ Disk-based caching (via `diskcache`) to avoid reprocessing same audio
- ğŸ§  Multiprocessing architecture for parallel decoding
- ğŸ–¥ï¸ GPU and CPU modes supported (automatically selected)

---

## ğŸš€ How to Run

### 1. Install dependencies

```bash
pip install -r requirements.txt
````

**Or manually:**

```bash
pip install fastapi uvicorn faster-whisper diskcache
```

If using GPU:

* Make sure you have installed CUDA and cuDNN (e.g. CUDA 12.4)
* `torch.cuda.is_available()` should return `True`

---

### 2. Start the server

```bash
python main.py
```

By default, the server runs at `http://localhost:8000`

---

## ğŸ“¡ API Usage

### `POST /transcribe`

Transcribe any audio file using a chosen model.

#### Request

* `file`: audio file (`.mp3`, `.wav`, etc.)
* `model`: model name (default: `base`)

#### Example using curl

```bash
curl.exe -F "file=@C:/path/to/audio.mp3" "http://localhost:8000/transcribe?model=base"
```

#### Example Response

```json
{
  "text": "This is the transcribed result."
}
```

---

## ğŸ§  Caching

The server uses `diskcache` to avoid redundant transcription for the same file. Cached results are keyed by the SHA256 hash of the audio + model name.

Cached files are stored in `./whisper_cache`. You can manually clean it or set expiration rules.

---

## ğŸ”§ Model Priority

Requests using smaller models like `tiny` and `base` are processed before heavier models like `large-v2`.

You can customize priorities in the `MODEL_PRIORITY` dictionary.

---

## ğŸ›‘ Shutdown Handling

The server cleanly shuts down:

* Cancels the response listener
* Terminates the model process
* Cleans up temporary files

---

## ğŸ’¡ TODO Ideas

* Add `/status` endpoint to monitor queue size and model usage
* Add TTL-based expiration to cache
* Support streaming or chunked transcription

---

## License

MIT â€” use it, modify it, profit from it. No warranty.


