–í–æ—Ç –º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π `README.md` –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –¥–ª—è —Ç–≤–æ–µ–≥–æ FastAPI + Faster Whisper —Å–µ—Ä–≤–∏—Å–∞:

---

````markdown
# WhisperServer

A high-performance transcription server using [Faster Whisper](https://github.com/guillaumekln/faster-whisper), built with FastAPI, supporting:

- ‚úÖ On-the-fly model selection (`tiny`, `base`, `small`, etc.)
- ‚ö° Priority-based queue (lightweight models are processed first)
- üíæ Disk-based caching (via `diskcache`) to avoid reprocessing same audio
- üß† Multiprocessing architecture for parallel decoding
- üñ•Ô∏è GPU and CPU modes supported (automatically selected)

---

## üöÄ How to Run

### 1. Install dependencies

```bash
pip install -r requirements.txt
````

**Or manually:**

```bash
pip install fastapi uvicorn faster-whisper diskcache
```

If using GPU:

* Make sure you have installed CUDA and cuDNN (e.g. CUDA 12.4)
* `torch.cuda.is_available()` should return `True`

---

### 2. Start the server

```bash
python main.py
```

By default, the server runs at `http://localhost:8000`

---

## üì° API Usage

### `POST /transcribe`

Transcribe any audio file using a chosen model.

#### Request

* `file`: audio file (`.mp3`, `.wav`, etc.)
* `model`: model name (default: `base`)

#### Example using curl

```bash
curl.exe -F "file=@C:/path/to/audio.mp3" "http://localhost:8000/transcribe?model=base"
```

#### Example Response

```json
{
  "text": "This is the transcribed result."
}
```

---

## üß† Caching

The server uses `diskcache` to avoid redundant transcription for the same file. Cached results are keyed by the SHA256 hash of the audio + model name.

Cached files are stored in `./whisper_cache`. You can manually clean it or set expiration rules.

---

## üîß Model Priority

Requests using smaller models like `tiny` and `base` are processed before heavier models like `large-v2`.

You can customize priorities in the `MODEL_PRIORITY` dictionary.

---

## üõë Shutdown Handling

The server cleanly shuts down:

* Cancels the response listener
* Terminates the model process
* Cleans up temporary files

---

## üí° TODO Ideas

* Add `/status` endpoint to monitor queue size and model usage
* Add TTL-based expiration to cache
* Support streaming or chunked transcription

---

## License

MIT ‚Äî use it, modify it, profit from it. No warranty.


